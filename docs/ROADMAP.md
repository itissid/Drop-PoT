# Jargon
Event: I call every thing we want to do an event. Sleeping or Skydiving.
Context Engine: Presence and other things around you.
BFF: The AI that helps you behind the scene to "Plan an evening".
Topic Models: NLP clustering of words/sentences.


# For the completion of the Hoboken/Bangalore Demo: 

1. Complete the geospatial index example for our little dataset. The data processing of this is easier: Just convert the addresses to lat longs. Using a  database like SQLite backed R-Tree is ok for data extraction and a demo but in prod we need a more robust solution for scaling geo spatial query.

2. Define a BFFEngine. The key responsibilities of this engine are 
- Take the set of documents and a Context Object and polish the data according to the  

3. Define a ContextEngine class that can store and load context using a simple API. This will be the interface to
load and store context which is 
- Location(approx or precise)
- Date and time.
- Weather at the time
- Some mood from the user??? This is in the demo but we don't ask them that we just show all the moods and the prompts in each mood modal. 
- Other deets like demographics or nature of the place(like suburbs vs towns)?
- User preference for what item they liked or not(Thumbs up and down).
- Implicit context: Like System prompt instructions for the BFF depending on place. 

4. Add an image database: We can for starters keep it simple. Lets just serve the images that are related to an Event.
I think serving images fast via a CDN is a key challenge for UX. But what more should we do with images for the Prompts?

# For model improvement 
## Data scraping
Two things here are key:
- We want to know how our toy model performs in terms of quality.
- We want to define a quality metric for improving this model.
- Find scrapers for Google Restaurant/Facebook/Insta pages which we can use for getting Place review data.

1. The data: Venky and I will work on ingesting it into the framework then try generate some moods from chatGPT and index
them into our database and check them manually. 
2. We only want moods that have a good quality. Here is a summary from ChatGPT on how to approach a metric from quality improvement for these moods(or really any prompt category that clusters text data), where we should start:


    - Topic models, such as Latent Dirichlet Allocation (LDA), are probabilistic models used to uncover hidden structures (topics) in a collection of documents. They can provide a high-level overview of a large set of documents and identify recurring themes. However, not all topics generated by these models are of the same quality or coherence. Therefore, several metrics have been developed to assess the quality of the topics generated by topic models.

        - Perplexity: This is one of the most common metrics for evaluating topic models. It is based on the probability of observing a test set of documents given the model estimated on a training set of documents. Lower perplexity means the model is better at predicting the test set and thus is considered to be better.

        - Topic Coherence: This measures the degree of semantic similarity between high scoring words in the topic. Topics with higher coherence scores are considered to be of better quality. There are several measures of coherence, such as UMass, c_v, c_npmi, and c_uci, which differ mainly in how they calculate word similarity.

        - Topic Diversity: This measures how different the topics are from each other. It is calculated as the average pairwise similarity of topics, with lower scores indicating more diverse (and thus likely higher-quality) topics.

        - Word Intrusion: This is a human-evaluated measure. For each topic, a word that does not belong to the topic is added (intruded), and the words are shuffled. People are then asked to identify the intruded word. If people can correctly identify the intruder, the original words are likely from the same topic.

        - Topic Intrusion: Like word intrusion, but for topics. A document is chosen, and an extra topic that is not related to the document is added. If people can correctly identify the intruded topic, the original topics are likely relevant to the document.

        - Exclusivity: This metric looks at the exclusivity of words to topics, which can provide insight into the quality of the topic. If a word appears in many topics, it's not very exclusive and may lead to a lack of distinctiveness between topics.

        - Manual Evaluation: Ultimately, one of the best ways to judge the quality of a topic model is through human evaluation. While this can be time-consuming, it allows for a qualitative understanding of how well the topics capture meaningful themes in the data.

    > Remember that while these metrics can help gauge the quality of the topics produced by a model, they should not be used in isolation. The best metric or combination of metrics may depend on the specific application and data set. And ultimately, a good topic model should produce topics that make intuitive sense for the given collection of documents.


The hardest part is to drive  direction of the team using data and metrics. Since we have less idea of what should happen next: Like should we pilot with text only prompots? or should we fitz around with image data  to improve engagement or something else?